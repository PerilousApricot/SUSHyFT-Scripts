SUSHyFT
=======

Scripts used for wrangling results out of 2012 CMS data. There are a few
processing steps involved between the CMS-provided data and the needed
input for the final fitting/plotting steps. Most of this code is made to
handle that.

Workflow
--------

The pipeline works (nearly) as follows:
0. RAW to RECO to AODSIM - Produced centerally by CMS. The starting point for all analyses (~1PB)
1. AOD[SIM] to PAT - Handled by tlbsm v3 PAT sequence. Stores fat PAT objects per-event. Generated using CRAB. The "input" for the analysis (~100TB)
2. Calculate Luminosity - Dump the CRAB luminosity information for the PAT dataset and store the integrated luminosity to later scale the Monte Carlo background 
3. PAT to EDNTuple - Stores simplified physics objects per event (eg Pt/eta/phi for leading jets). Generated using CRAB (~10TB)
4. EDNtuple to FWLite - From the per-event information, generate histograms per jet/tag bin (eg Pt of leading jet in events with 1jet, 1b-tag, 0tau bin). Generated by parallelizing over PBS. (~100GB)
5. Combining FWLite histograms - Each FWLite job produces a file, hadd every file for a given dataset to a single file. Generated on single node interactively (~1GB)
6. Rebin histograms - Each hadded FWLite job has a histogram for every possible combination of jet/btag/tautag. To support specific analysis configurations, drop/combine different bins together (eg drop 0-jet bins, combine all bins with nJets >= 5) (100MB)
7. Stitch histograms - Combine all the relevant datasets for a given analysis configuration into one file per-systemati (eg all the desired data/signal/background histograms). At this point, we scale the Monte Carlo estimations by the theoretical cross sections, with exception to the QCD background, which is handled seperately (~1MB)
8. Copy histograms - Do one final pass over the input histograms, rebinning them and combinining the flavor distributions together. (~1MB)
9. Generate QCD normalization - To more accurately normalize the QCD background, a fit against the full MET spectrum is performed. This QCD normalization is then propagated to the full fitter.
10. Generate polynoids - Using different input samples with various systematic values shifted up/down 1/2 sigma, generate a series of polynomials that describe per-sample and per-jet/tag bin the relative scaling given a specific value of a systematic parameter.
11. Compute fits. Using the input templates from the copy histograms step along with the QCD fit and systematic polynoids, Fit the signal and backgrounds to the data

Directory structure
-------------------

* bin/ - Compiled binaries. Added to $PATH by sourcing environment
* checkouts/ - CMSSW checkout locations
* config/ - Input configuration files
* data/ - Generated data
* python/ - Added to $PYTHONPATH by sourcing the environment
* scripts/ - Added to $PATH by sourcing the environment
* topLevelScripts/ - Metascripts to run whole steps at once. Shouldn't accept any parameters. Instead of documenting command line arguments, the framework will
